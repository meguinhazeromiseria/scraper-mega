#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SUPERBID - SCRAPER COMPLETO DO DOM√çNIO
Varre TODO o site Superbid via API REST
Scrape ‚Üí Normalize ‚Üí Classify ‚Üí Insert
"""

import sys
import json
import time
import random
import requests
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import List, Optional

# Adiciona pasta pai ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from supabase_client import SupabaseClient
from groq_classifier import GroqTableClassifier
from normalizer import normalize_items


class SuperbidScraper:
    """Scraper completo do dom√≠nio Superbid via API REST"""
    
    def __init__(self):
        self.source = 'superbid'
        self.base_url = 'https://offer-query.superbid.net/seo/offers/'
        self.site_url = 'https://exchange.superbid.net'
        self.session = requests.Session()
        
        # Todas as se√ß√µes do Superbid
        # Para ve√≠culos: usa subcategorias espec√≠ficas com vehicle_type
        # Para outras: usa categoria principal
        self.main_sections = [
            # VE√çCULOS (subcategorias espec√≠ficas para manter vehicle_type)
            ('carros-motos/carros', 'Carros', {'vehicle_type': 'carro'}),
            ('carros-motos/motos', 'Motos', {'vehicle_type': 'moto'}),
            ('caminhoes-onibus/caminhoes', 'Caminh√µes', {'vehicle_type': 'caminhao'}),
            ('caminhoes-onibus/onibus', '√înibus', {'vehicle_type': 'onibus'}),
            ('caminhoes-onibus/vans', 'Vans', {'vehicle_type': 'van'}),
            
            # OUTRAS CATEGORIAS (sem vehicle_type)
            ('embarcacoes-aeronaves', 'Embarca√ß√µes e Aeronaves', {}),
            ('imoveis', 'Im√≥veis', {}),
            ('tecnologia', 'Tecnologia', {}),
            ('eletrodomesticos', 'Eletrodom√©sticos', {}),
            ('industrial-maquinas-equipamentos', 'Industrial, M√°quinas e Equipamentos', {}),
            ('maquinas-pesadas-agricolas', 'M√°quinas Pesadas e Agr√≠colas', {}),
            ('materiais-para-construcao-civil', 'Materiais para Constru√ß√£o Civil', {}),
            ('moveis-e-decoracao', 'M√≥veis e Decora√ß√£o', {}),
            ('cozinhas-e-restaurantes', 'Cozinhas e Restaurantes', {}),
            ('movimentacao-transporte', 'Movimenta√ß√£o e Transporte', {}),
            ('partes-e-pecas', 'Partes e Pe√ßas', {}),
            ('sucatas-materiais-residuos', 'Sucatas, Materiais e Res√≠duos', {}),
            ('alimentos-e-bebidas', 'Alimentos e Bebidas', {}),
            ('animais', 'Animais', {}),
            ('artes-decoracao-colecionismo', 'Artes, Decora√ß√£o e Colecionismo', {}),
            ('bolsas-canetas-joias-e-relogios', 'Bolsas, Canetas, Joias e Rel√≥gios', {}),
            ('oportunidades', 'Oportunidades', {}),
        ]
        
        self.stats = {
            'total_scraped': 0,
            'by_section': {},
            'duplicates': 0,
        }
        
        # Headers padr√£o para API
        self.headers = {
            "accept": "*/*",
            "accept-language": "pt-BR,pt;q=0.9",
            "origin": "https://exchange.superbid.net",
            "referer": "https://exchange.superbid.net/",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        }
    
    def scrape(self) -> List[dict]:
        """Scrape completo do Superbid"""
        print("\n" + "="*60)
        print("üîµ SUPERBID - DOM√çNIO COMPLETO")
        print("="*60)
        
        all_items = []
        global_ids = set()
        
        # Varre cada se√ß√£o principal
        for section_slug, section_name, extra_fields in self.main_sections:
            print(f"\nüì¶ Se√ß√£o: {section_name}")
            section_items = self._scrape_section(section_slug, section_name, extra_fields, global_ids)
            
            all_items.extend(section_items)
            self.stats['by_section'][section_slug] = len(section_items)
            
            print(f"‚úÖ {len(section_items)} itens coletados")
            
            # Delay entre se√ß√µes
            time.sleep(2)
        
        self.stats['total_scraped'] = len(all_items)
        return all_items
    
    def _scrape_section(self, section_slug: str, section_name: str, extra_fields: dict, global_ids: set) -> List[dict]:
        """Scrape uma se√ß√£o completa do Superbid via API"""
        items = []
        page_num = 1
        page_size = 100
        consecutive_errors = 0
        max_errors = 3
        max_pages = 100
        
        while page_num <= max_pages and consecutive_errors < max_errors:
            print(f"  P√°g {page_num}", end='', flush=True)
            
            try:
                # Par√¢metros da API (baseado no c√≥digo que funcionava)
                params = {
                    "urlSeo": f"https://exchange.superbid.net/categorias/{section_slug}",
                    "locale": "pt_BR",
                    "orderBy": "offerDetail.percentDiffReservedPriceOverFipePrice:asc",
                    "pageNumber": page_num,
                    "pageSize": page_size,
                    "portalId": "[2,15]",
                    "preOrderBy": "orderByFirstOpenedOffersAndSecondHasPhoto",
                    "requestOrigin": "marketplace",
                    "searchType": "openedAll",
                    "timeZoneId": "America/Sao_Paulo",
                }
                
                # Request na API
                response = self.session.get(
                    self.base_url,
                    params=params,
                    headers=self.headers,
                    timeout=45
                )
                
                # Tratamento de erros
                if response.status_code == 404:
                    print(f" ‚ö™ Fim (404)")
                    break
                
                if response.status_code != 200:
                    print(f" ‚ö†Ô∏è Status {response.status_code}")
                    consecutive_errors += 1
                    time.sleep(5)
                    page_num += 1
                    continue
                
                data = response.json()
                offers = data.get("offers", [])
                
                if not offers:
                    print(f" ‚ö™ Vazia")
                    break
                
                novos = 0
                duplicados = 0
                
                for offer in offers:
                    item = self._extract_offer(offer, section_slug, section_name, extra_fields)
                    
                    if not item:
                        continue
                    
                    # Verifica duplicata
                    if item['external_id'] in global_ids:
                        duplicados += 1
                        self.stats['duplicates'] += 1
                        continue
                    
                    items.append(item)
                    global_ids.add(item['external_id'])
                    novos += 1
                
                if novos > 0:
                    print(f" ‚úÖ +{novos} | Total se√ß√£o: {len(items)}")
                    consecutive_errors = 0
                else:
                    print(f" ‚ö™ 0 novos (dup: {duplicados})")
                
                # Verifica se √© √∫ltima p√°gina
                if len(offers) < page_size:
                    print("  ‚úÖ √öltima p√°gina")
                    break
                
                page_num += 1
                time.sleep(random.uniform(2, 5))
                
            except requests.exceptions.JSONDecodeError:
                print(f" ‚ö†Ô∏è Erro JSON")
                consecutive_errors += 1
                time.sleep(5)
                page_num += 1
            
            except Exception as e:
                print(f" ‚ùå Erro: {str(e)[:80]}")
                consecutive_errors += 1
                time.sleep(5)
                page_num += 1
        
        return items
    
    def _extract_offer(self, offer: dict, section_slug: str, section_name: str, extra_fields: dict) -> Optional[dict]:
        """
        Extrai dados da oferta Superbid.
        N√ÉO decide categoria final - apenas coleta dados brutos.
        Mant√©m vehicle_type quando dispon√≠vel (ve√≠culos).
        """
        try:
            # Estrutura da resposta da API
            product = offer.get("product", {})
            auction = offer.get("auction", {})
            detail = offer.get("offerDetail", {})
            seller = offer.get("seller", {})
            store = offer.get("store", {})
            
            # ID externo
            offer_id = str(offer.get("id"))
            if not offer_id:
                return None
            
            external_id = f"superbid_{offer_id}"
            
            # T√≠tulo
            title = (product.get("shortDesc") or "").strip()
            if not title or len(title) < 3:
                return None
            
            # Descri√ß√£o completa
            full_desc = offer.get("offerDescription", {}).get("offerDescription", "")
            description_preview = full_desc[:200] if full_desc else title[:200]
            
            # Valor
            value = detail.get("currentMinBid") or detail.get("initialBidValue")
            value_text = detail.get("currentMinBidFormatted") or detail.get("initialBidValueFormatted")
            
            # Localiza√ß√£o (formato: "Cidade/UF" ou "Cidade - UF")
            city = None
            state = None
            seller_city = seller.get("city", "") or ""
            
            if '/' in seller_city:
                parts = seller_city.split('/')
                city = parts[0].strip()
                state = parts[1].strip() if len(parts) > 1 else None
            elif ' - ' in seller_city:
                parts = seller_city.split(' - ')
                city = parts[0].strip()
                state = parts[1].strip() if len(parts) > 1 else None
            
            # Valida UF
            if state and (len(state) != 2 or not state.isupper()):
                state = None
            
            # Link
            link = f"https://exchange.superbid.net/oferta/{offer_id}"
            
            # Data do leil√£o
            auction_date = None
            end_date_str = offer.get("endDate")
            if end_date_str:
                try:
                    auction_date = datetime.fromisoformat(end_date_str.replace('Z', '+00:00'))
                except:
                    pass
            
            # ‚úÖ MONTA ITEM BASE - SEM DECIDIR CATEGORIA
            item = {
                'source': 'superbid',
                'external_id': external_id,
                'title': title,
                'description': full_desc,
                'description_preview': description_preview,
                'value': value,
                'value_text': value_text,
                'city': city,
                'state': state,
                'link': link,
                
                # Categoria ORIGINAL do site (s√≥ metadata, n√£o decis√£o)
                'raw_category': section_slug,
                
                'metadata': {
                    'secao_site': section_name,
                    'secao_slug': section_slug,
                    'leilao_tipo': auction.get("modalityDesc"),
                    'leilao_nome': auction.get("desc"),
                    'leiloeiro': auction.get("auctioneer"),
                    'loja_nome': store.get("name"),
                    'vendedor': seller.get("name"),
                    'lote_numero': offer.get("lotNumber"),
                    'total_visitas': offer.get("visits", 0),
                    'total_lances': offer.get("totalBids", 0),
                    'total_participantes': offer.get("totalBidders", 0),
                    'data_leilao': auction_date.isoformat() if auction_date else None,
                }
            }
            
            # ‚úÖ ADICIONA VEHICLE_TYPE APENAS PARA VE√çCULOS
            # Isso ajuda os handlers de busca, mas N√ÉO define a tabela final
            if 'vehicle_type' in extra_fields:
                item['vehicle_type'] = extra_fields['vehicle_type']
            
            # Filtra itens de teste/demo
            store_name = str(store.get("name", "")).lower()
            if not store.get("name") or 'demo' in store_name or 'test' in store_name:
                return None
            
            # Valor muito baixo (suspeito)
            if value and value < 1:
                return None
            
            return item
            
        except Exception as e:
            # Silencioso - n√£o loga cada erro de parsing
            return None


def main():
    """Execu√ß√£o principal"""
    print("\n" + "="*70)
    print("üöÄ SUPERBID - SCRAPER COMPLETO DO DOM√çNIO")
    print("="*70)
    print(f"üìÖ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    start_time = time.time()
    
    # ========================================
    # FASE 1: SCRAPE
    # ========================================
    print("\nüî• FASE 1: COLETANDO DADOS")
    scraper = SuperbidScraper()
    items = scraper.scrape()
    
    print(f"\n‚úÖ Total coletado: {len(items)} itens")
    print(f"üîÑ Duplicatas filtradas: {scraper.stats['duplicates']}")
    
    if not items:
        print("‚ö†Ô∏è Nenhum item coletado - encerrando")
        return
    
    # ========================================
    # FASE 2: NORMALIZA√á√ÉO
    # ========================================
    print("\n‚ú® FASE 2: NORMALIZANDO DADOS")
    try:
        normalized_items = normalize_items(items)
        print(f"‚úÖ {len(normalized_items)} itens normalizados")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro na normaliza√ß√£o: {e}")
        print("Usando dados brutos...")
        normalized_items = items
    
    # Salva JSON normalizado (para debug)
    output_dir = Path(__file__).parent / 'data' / 'normalized'
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_file = output_dir / f'superbid_{timestamp}.json'
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(normalized_items, f, ensure_ascii=False, indent=2)
    print(f"üíæ JSON salvo: {json_file}")
    
    # ========================================
    # FASE 3: CLASSIFICA√á√ÉO GROQ
    # ========================================
    print("\nü§ñ FASE 3: CLASSIFICANDO COM GROQ AI")
    try:
        classifier = GroqTableClassifier()
        items_by_table = defaultdict(list)
        
        for i, item in enumerate(normalized_items, 1):
            if i % 10 == 0:
                print(f"  ‚è≥ {i}/{len(normalized_items)}")
            
            table = classifier.classify(item)
            if table:
                items_by_table[table].append(item)
            
            time.sleep(0.2)  # Rate limit Groq
        
        print(f"‚úÖ Classifica√ß√£o conclu√≠da!")
        print(f"\nüìä Distribui√ß√£o por tabela:")
        for table, table_items in sorted(items_by_table.items()):
            print(f"  ‚Ä¢ {table}: {len(table_items)} itens")
        
        # Print stats do classifier
        classifier.print_stats()
    
    except Exception as e:
        print(f"‚ö†Ô∏è Erro na classifica√ß√£o: {e}")
        print("Colocando tudo em 'oportunidades'...")
        items_by_table = {'oportunidades': normalized_items}
    
    # ========================================
    # FASE 4: INSERT NO SUPABASE
    # ========================================
    print("\nüì§ FASE 4: INSERINDO NO SUPABASE")
    try:
        supabase = SupabaseClient()
        
        if not supabase.test():
            print("‚ö†Ô∏è Erro na conex√£o com Supabase - pulando insert")
        else:
            total_inserted = 0
            total_updated = 0
            
            for table, table_items in items_by_table.items():
                if not table_items:
                    continue
                
                print(f"\n  üì§ Tabela '{table}': {len(table_items)} itens")
                stats = supabase.upsert(table, table_items)
                
                print(f"    ‚úÖ Inseridos: {stats['inserted']}")
                print(f"    üîÑ Atualizados: {stats['updated']}")
                if stats['errors'] > 0:
                    print(f"    ‚ö†Ô∏è Erros: {stats['errors']}")
                
                total_inserted += stats['inserted']
                total_updated += stats['updated']
            
            print(f"\n  üìà TOTAL:")
            print(f"    ‚úÖ Inseridos: {total_inserted}")
            print(f"    üîÑ Atualizados: {total_updated}")
    
    except Exception as e:
        print(f"‚ö†Ô∏è Erro no Supabase: {e}")
    
    # ========================================
    # ESTAT√çSTICAS FINAIS
    # ========================================
    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    
    print("\n" + "="*70)
    print("üìä ESTAT√çSTICAS FINAIS")
    print("="*70)
    print(f"üîµ Superbid - Dom√≠nio Completo:")
    print(f"\n  Por Se√ß√£o do Site:")
    for section, count in sorted(scraper.stats['by_section'].items()):
        print(f"    ‚Ä¢ {section}: {count} itens")
    print(f"\n  ‚Ä¢ Total coletado: {scraper.stats['total_scraped']}")
    print(f"  ‚Ä¢ Duplicatas: {scraper.stats['duplicates']}")
    print(f"\n‚è±Ô∏è Dura√ß√£o: {minutes}min {seconds}s")
    print(f"‚úÖ Conclu√≠do: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)


if __name__ == "__main__":
    main()