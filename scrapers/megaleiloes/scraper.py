#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MEGALEIL√ïES - SCRAPER COMPLETO E CORRIGIDO
‚úÖ Pagina√ß√£o autom√°tica detectando bot√£o "Fim"
‚úÖ Extrai data, lances e imagem corretamente
‚úÖ Compat√≠vel 100% com tabela megaleiloes_items
‚úÖ Usa ?pagina=N (n√£o ?page=N)
"""

import sys
import json
import time
import re
import os
from pathlib import Path
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional

from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# Adiciona o diret√≥rio atual ao path para importar supabase_client
sys.path.insert(0, str(Path(__file__).parent))


def convert_brazilian_datetime_to_postgres(date_str: str) -> Optional[str]:
    """Converte data brasileira DD/MM/YYYY HH:MM para PostgreSQL ISO format"""
    try:
        date_str = date_str.replace('√†s', '').strip()
        dt = datetime.strptime(date_str, '%d/%m/%Y %H:%M')
        dt_with_tz = dt.replace(tzinfo=ZoneInfo('America/Sao_Paulo'))
        return dt_with_tz.isoformat()
    except Exception:
        return None


class MegaLeiloesScraper:
    """Scraper para MegaLeil√µes com pagina√ß√£o autom√°tica"""
    
    def __init__(self):
        """Inicializa scraper"""
        self.source = 'megaleiloes'
        self.base_url = 'https://www.megaleiloes.com.br'
        
        # Se√ß√µes principais
        self.sections = [
            ('imoveis', 'Im√≥veis', 'Im√≥veis'),
            ('veiculos', 'Ve√≠culos', 'Ve√≠culos'),
            ('bens-de-consumo', 'Bens de Consumo', 'Bens de Consumo'),
            ('industrial', 'Industrial', 'Industrial'),
            ('animais', 'Animais', 'Animais'),
            ('outros', 'Outros', 'Outros'),
        ]
        
        self.stats = {
            'total_scraped': 0,
            'by_category': {},
            'duplicates': 0,
            'with_bids': 0,
            'with_images': 0,
            'pages_scraped': 0,
        }
        
        # Estados brasileiros v√°lidos
        self.valid_states = [
            'AC','AL','AP','AM','BA','CE','DF','ES','GO','MA','MT','MS','MG',
            'PA','PB','PR','PE','PI','RJ','RN','RS','RO','RR','SC','SP','SE','TO'
        ]
    
    def scrape(self) -> List[Dict]:
        """Scrape completo do MegaLeil√µes"""
        print("\n" + "="*70)
        print("üü¢ MEGALEIL√ïES - SCRAPER COMPLETO")
        print("="*70)
        
        all_items = []
        global_ids = set()
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True, args=['--no-sandbox'])
                
                context = browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    viewport={'width': 1920, 'height': 1080},
                    locale='pt-BR'
                )
                
                page = context.new_page()
                
                for url_path, category, display_name in self.sections:
                    print(f"\n{'='*70}")
                    print(f"üì¶ {display_name}")
                    print(f"{'='*70}")
                    
                    section_items = self._scrape_section(
                        page, url_path, category, display_name, global_ids
                    )
                    
                    all_items.extend(section_items)
                    self.stats['by_category'][category] = len(section_items)
                    
                    print(f"‚úÖ {len(section_items)} itens coletados de {display_name}")
                    
                    time.sleep(2)
                
                browser.close()
        
        except Exception as e:
            print(f"‚ùå Erro geral: {e}")
            import traceback
            traceback.print_exc()
        
        self.stats['total_scraped'] = len(all_items)
        return all_items
    
    def _get_max_page(self, soup) -> int:
        """Detecta o n√∫mero m√°ximo de p√°ginas pelo bot√£o 'Fim'"""
        try:
            # Procura pelo bot√£o "Fim" na pagina√ß√£o
            last_link = soup.select_one('ul.pagination li.last a')
            if last_link:
                href = last_link.get('href', '')
                # Extrai n√∫mero da p√°gina do URL
                match = re.search(r'pagina=(\d+)', href)
                if match:
                    return int(match.group(1))
            
            # Se n√£o encontrar, tenta pelos links de p√°gina
            page_links = soup.select('ul.pagination li a[data-page]')
            if page_links:
                pages = []
                for link in page_links:
                    href = link.get('href', '')
                    match = re.search(r'pagina=(\d+)', href)
                    if match:
                        pages.append(int(match.group(1)))
                if pages:
                    return max(pages)
            
            return 1
            
        except Exception:
            return 1
    
    def _scrape_section(self, page, url_path: str, category: str,
                       display_name: str, global_ids: set) -> List[Dict]:
        """Scrape uma se√ß√£o espec√≠fica - todas as p√°ginas"""
        items = []
        
        # Primeiro acessa a p√°gina 1 para descobrir quantas p√°ginas existem
        url = f"{self.base_url}/{url_path}"
        
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            time.sleep(3)
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')
            
            # Detecta o n√∫mero m√°ximo de p√°ginas
            max_page = self._get_max_page(soup)
            print(f"üìÑ Total de p√°ginas detectadas: {max_page}")
            
            # Agora scrape todas as p√°ginas
            for page_num in range(1, max_page + 1):
                if page_num == 1:
                    current_url = url
                    current_soup = soup
                else:
                    current_url = f"{url}?pagina={page_num}"
                    page.goto(current_url, wait_until="domcontentloaded", timeout=60000)
                    time.sleep(3)
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
                    current_html = page.content()
                    current_soup = BeautifulSoup(current_html, 'html.parser')
                
                # Extrai cards
                cards = current_soup.select('div.card')
                
                if not cards:
                    print(f"  ‚ö†Ô∏è P√°gina {page_num}/{max_page}: Nenhum card encontrado")
                    continue
                
                print(f"  üìÑ P√°gina {page_num}/{max_page}: {len(cards)} cards encontrados")
                
                page_items = 0
                for card in cards:
                    item = self._parse_card(card, category)
                    
                    if item and item['external_id'] not in global_ids:
                        items.append(item)
                        global_ids.add(item['external_id'])
                        page_items += 1
                        
                        if item.get('has_bid'):
                            self.stats['with_bids'] += 1
                        
                        if item.get('image_url'):
                            self.stats['with_images'] += 1
                    elif item:
                        self.stats['duplicates'] += 1
                
                self.stats['pages_scraped'] += 1
                print(f"  ‚úÖ {page_items} itens v√°lidos extra√≠dos da p√°gina {page_num}")
                
                # Delay entre p√°ginas
                time.sleep(2)
        
        except Exception as e:
            print(f"‚ùå Erro ao processar se√ß√£o: {e}")
            import traceback
            traceback.print_exc()
        
        return items
    
    def _parse_card(self, card, category: str) -> Optional[Dict]:
        """Parse de um card de leil√£o"""
        try:
            # 1. Extrai link
            link_elem = card.select_one('a[href]')
            if not link_elem:
                return None
            
            link = link_elem.get('href', '')
            if not link or 'javascript' in link.lower():
                return None
            
            if not link.startswith('http'):
                link = f"{self.base_url}{link}"
            
            # Remove par√¢metros UTM
            link_clean = link.split('?')[0].rstrip('/')
            
            # 2. Extrai external_id do link
            external_id = None
            parts = link_clean.split('/')
            for part in reversed(parts):
                if part and not part.startswith('?'):
                    external_id = f"{self.source}_{part}"
                    break
            
            if not external_id or external_id == f'{self.source}_':
                return None
            
            # 3. Extrai texto completo
            texto = card.get_text(separator=' ', strip=True)
            
            # Filtra cards muito curtos
            if len(texto) < 20:
                return None
            
            # 4. T√≠tulo (prioriza .card-title)
            title_elem = card.select_one('.card-title')
            if title_elem:
                title = title_elem.get_text(strip=True)
            else:
                # Pega primeiras palavras do texto
                words = texto.split()[:15]
                title = ' '.join(words)
            
            # 5. Imagem (data-bg do a.card-image)
            image_url = None
            image_elem = card.select_one('a.card-image[data-bg]')
            if image_elem:
                image_url = image_elem.get('data-bg')
                # Filtra imagem padr√£o "no-image"
                if image_url and 'no-image' in image_url:
                    image_url = None
            
            # 6. Extrai informa√ß√µes de pra√ßa
            auction_info = self._extract_auction_info_from_html(card)
            
            # 7. Has bid (√≠cone fa-legal)
            has_bid = self._extract_has_bid(card)
            
            # 8. Valor
            value = auction_info.get('current_value')
            value_text = auction_info.get('current_value_text')
            
            if not value:
                price_elem = card.select_one('.card-price')
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price_match = re.search(r'R\$\s*([\d.]+,\d{2})', price_text)
                    if price_match:
                        value_text = f"R$ {price_match.group(1)}"
                        try:
                            value = float(price_match.group(1).replace('.', '').replace(',', '.'))
                        except:
                            pass
            
            # 9. Cidade e Estado (usa .card-locality se dispon√≠vel)
            city = None
            state = None
            
            locality_elem = card.select_one('.card-locality')
            if locality_elem:
                locality_text = locality_elem.get_text(strip=True)
                # Formato: "S√£o Jo√£o Del Rei, MG"
                match = re.match(r'^(.+),\s*([A-Z]{2})$', locality_text)
                if match:
                    city = match.group(1).strip()
                    state = match.group(2).strip()
            
            # Se n√£o encontrou, tenta no texto geral
            if not city or not state:
                city_match = re.search(r'([A-Z√Ä-√ö][a-z√†-√∫]+(?:\s+[A-Z√Ä-√ö][a-z√†-√∫]+)*)\s*,\s*([A-Z]{2})\b', texto)
                if city_match:
                    if not city:
                        city = city_match.group(1).strip()
                    if not state:
                        state = city_match.group(2)
            
            # 10. Tipo de leil√£o (usa .card-instance-title a)
            auction_type = None
            type_elem = card.select_one('.card-instance-title a')
            if type_elem:
                type_text = type_elem.get_text(strip=True)
                if 'judicial' in type_text.lower():
                    auction_type = 'Judicial'
                elif 'extrajudicial' in type_text.lower():
                    auction_type = 'Extrajudicial'
            
            # Se n√£o encontrou, busca no texto
            if not auction_type:
                if 'judicial' in texto.lower():
                    auction_type = 'Judicial'
                elif 'extrajudicial' in texto.lower():
                    auction_type = 'Extrajudicial'
            
            # 11. N√∫mero do lote (card-number)
            batch_number = None
            number_elem = card.select_one('.card-number')
            if number_elem:
                batch_number = number_elem.get_text(strip=True)
            
            # 12. Constr√≥i o item compat√≠vel com DB
            item = {
                'source': self.source,
                'external_id': external_id,
                'category': category,
                'title': title,
                'description': texto,
                'city': city,
                'state': state,
                'value': value,
                'value_text': value_text,
                'auction_round': auction_info.get('auction_round'),
                'auction_date': auction_info.get('auction_date'),
                'first_round_value': auction_info.get('first_round_value'),
                'first_round_date': auction_info.get('first_round_date'),
                'discount_percentage': auction_info.get('discount_percentage'),
                'link': link,
                'image_url': image_url,
                'metadata': {'batch_number': batch_number} if batch_number else {},
                'is_active': True,
                'has_bid': has_bid,
                'auction_type': auction_type,
            }
            
            return item
            
        except Exception:
            return None
    
    def _extract_has_bid(self, card) -> bool:
        """Verifica se o item tem lances - procura pelo √≠cone fa-legal"""
        try:
            legal_icon = card.select_one('i.fa-legal')
            
            if legal_icon:
                parent_span = legal_icon.find_parent('span')
                if parent_span:
                    text = parent_span.get_text(strip=True)
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        bid_count = int(numbers[0])
                        return bid_count > 0
            
            return False
            
        except Exception:
            return False
    
    def _extract_auction_info_from_html(self, card) -> Dict:
        """Extrai informa√ß√µes de pra√ßa do HTML"""
        info = {
            'auction_round': None,
            'auction_date': None,
            'current_value': None,
            'current_value_text': None,
            'first_round_value': None,
            'first_round_date': None,
            'discount_percentage': None,
        }
        
        # Pra√ßa ativa (atual)
        active_instance = card.select_one('.instance.active')
        
        if active_instance:
            # Verifica se √© segunda pra√ßa
            second_date = active_instance.select_one('.card-second-instance-date')
            first_date = active_instance.select_one('.card-first-instance-date')
            
            if second_date:
                info['auction_round'] = 2
                date_text = second_date.get_text(strip=True)
                date_match = re.search(r'(\d{2}/\d{2}/\d{4})\s*√†s\s*(\d{2}:\d{2})', date_text)
                if date_match:
                    date_str = f"{date_match.group(1)} {date_match.group(2)}"
                    info['auction_date'] = convert_brazilian_datetime_to_postgres(date_str)
                
            elif first_date:
                info['auction_round'] = 1
                date_text = first_date.get_text(strip=True)
                date_match = re.search(r'(\d{2}/\d{2}/\d{4})\s*√†s\s*(\d{2}:\d{2})', date_text)
                if date_match:
                    date_str = f"{date_match.group(1)} {date_match.group(2)}"
                    info['auction_date'] = convert_brazilian_datetime_to_postgres(date_str)
            
            # Valor atual
            value_elem = active_instance.select_one('.card-instance-value')
            if value_elem:
                value_text = value_elem.get_text(strip=True)
                info['current_value_text'] = value_text
                
                value_match = re.search(r'R\$\s*([\d.]+,\d{2})', value_text)
                if value_match:
                    try:
                        info['current_value'] = float(value_match.group(1).replace('.', '').replace(',', '.'))
                    except:
                        pass
        
        # Primeira pra√ßa (hist√≥rico)
        first_instance = card.select_one('.instance.first.passed')
        if first_instance:
            # Data da primeira pra√ßa
            date_elem = first_instance.select_one('.card-first-instance-date')
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                date_match = re.search(r'(\d{2}/\d{2}/\d{4})\s*√†s\s*(\d{2}:\d{2})', date_text)
                if date_match:
                    date_str = f"{date_match.group(1)} {date_match.group(2)}"
                    info['first_round_date'] = convert_brazilian_datetime_to_postgres(date_str)
            
            # Valor da primeira pra√ßa
            value_elem = first_instance.select_one('.card-instance-value')
            if value_elem:
                value_text = value_elem.get_text(strip=True)
                value_match = re.search(r'R\$\s*([\d.]+,\d{2})', value_text)
                if value_match:
                    try:
                        info['first_round_value'] = float(value_match.group(1).replace('.', '').replace(',', '.'))
                    except:
                        pass
        
        # Calcula desconto (se for segunda pra√ßa)
        if info['first_round_value'] and info['current_value'] and info['auction_round'] == 2:
            try:
                discount = ((info['first_round_value'] - info['current_value']) / info['first_round_value']) * 100
                info['discount_percentage'] = round(discount, 2)
            except:
                pass
        
        return info


def main():
    """Execu√ß√£o principal"""
    print("\n" + "="*70)
    print("üöÄ MEGALEIL√ïES - SCRAPER COMPLETO")
    print("="*70)
    print(f"üìÖ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    start_time = time.time()
    
    # Scrape
    scraper = MegaLeiloesScraper()
    items = scraper.scrape()
    
    print(f"\n{'='*70}")
    print(f"üìä RESULTADO FINAL")
    print(f"{'='*70}")
    print(f"‚úÖ Total coletado: {len(items)} itens")
    print(f"üìÑ P√°ginas processadas: {scraper.stats['pages_scraped']}")
    print(f"üñºÔ∏è Itens com imagens: {scraper.stats['with_images']}")
    print(f"üî• Itens com lances: {scraper.stats['with_bids']}")
    print(f"üîÑ Duplicatas filtradas: {scraper.stats['duplicates']}")
    
    if not items:
        print("\n‚ö†Ô∏è Nenhum item coletado - encerrando")
        return
    
    # Salva JSON
    output_dir = Path(__file__).parent / 'data'
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_file = output_dir / f'megaleiloes_{timestamp}.json'
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(items, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ JSON salvo: {json_file}")
    
    # Importa e usa o cliente Supabase
    try:
        # Verifica se as vari√°veis de ambiente est√£o configuradas
        if not os.getenv('SUPABASE_URL') or not os.getenv('SUPABASE_SERVICE_ROLE_KEY'):
            print("\n‚ö†Ô∏è Vari√°veis SUPABASE n√£o configuradas - pulando insert")
        else:
            from supabase_client import SupabaseMegaLeiloes
            
            print(f"\n{'='*70}")
            print("üì§ INSERINDO NO SUPABASE")
            print(f"{'='*70}")
            
            supabase = SupabaseMegaLeiloes()
            
            if not supabase.test():
                print("‚ö†Ô∏è Erro na conex√£o com Supabase - pulando insert")
            else:
                stats = supabase.upsert(items)
                
                print(f"\n  üìà RESULTADO:")
                print(f"    ‚úÖ Inseridos: {stats['inserted']}")
                print(f"    üîÑ Atualizados: {stats['updated']}")
                if stats['errors'] > 0:
                    print(f"    ‚ö†Ô∏è Erros: {stats['errors']}")
    
    except ImportError as e:
        print(f"\n‚ö†Ô∏è M√≥dulo supabase_client n√£o encontrado: {e}")
        print("   (JSON salvo, mas n√£o foi poss√≠vel inserir no banco)")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erro no Supabase: {e}")
        import traceback
        traceback.print_exc()
    
    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    
    print(f"\n{'='*70}")
    print("üìä ESTAT√çSTICAS FINAIS")
    print(f"{'='*70}")
    print(f"\n  Por Categoria:")
    for category, count in sorted(scraper.stats['by_category'].items()):
        print(f"    ‚Ä¢ {category}: {count} itens")
    print(f"\n  ‚Ä¢ Total coletado: {scraper.stats['total_scraped']}")
    print(f"  ‚Ä¢ P√°ginas processadas: {scraper.stats['pages_scraped']}")
    print(f"  ‚Ä¢ Com imagens: {scraper.stats['with_images']}")
    print(f"  ‚Ä¢ Com lances: {scraper.stats['with_bids']}")
    print(f"  ‚Ä¢ Duplicatas: {scraper.stats['duplicates']}")
    print(f"\n‚è±Ô∏è Dura√ß√£o: {minutes}min {seconds}s")
    print(f"‚úÖ Conclu√≠do: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*70}")


if __name__ == "__main__":
    main()